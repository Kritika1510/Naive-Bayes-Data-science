{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Ensemble techniques in machine learning involve combining multiple models to improve predictive performance. These models can be of the same type or different types, and their predictions are aggregated to make a final prediction.\n",
    "\n",
    "Q2. Ensemble techniques are used in machine learning because they often result in better predictive performance compared to individual models. By combining the predictions of multiple models, ensemble methods can reduce overfitting, increase robustness, and capture different aspects of the data.\n",
    "\n",
    "Q3. Bagging, or Bootstrap Aggregating, is an ensemble technique where multiple models are trained on different subsets of the training data, sampled with replacement. The final prediction is often obtained by averaging the predictions of all the models (for regression) or using a voting mechanism (for classification).\n",
    "\n",
    "Q4. Boosting is another ensemble technique where multiple weak learners are combined to create a strong learner. In boosting, each model in the ensemble focuses on the instances that previous models have misclassified, thereby sequentially improving the overall predictive performance.\n",
    "\n",
    "Q5. The benefits of using ensemble techniques include improved predictive performance, increased robustness, reduced overfitting, and the ability to capture complex relationships in the data. Ensemble methods also offer better generalization to unseen data and are less sensitive to noise.\n",
    "\n",
    "Q6. While ensemble techniques often outperform individual models, they are not always guaranteed to do so. The effectiveness of ensemble methods depends on various factors such as the quality and diversity of the base models, the nature of the data, and the specific problem at hand.\n",
    "\n",
    "Q7. The confidence interval using bootstrap is calculated by resampling the dataset with replacement multiple times to create a distribution of sample statistics (such as the mean). The confidence interval is then estimated from this distribution based on specified confidence level (e.g., 95%).\n",
    "\n",
    "Q8. Bootstrap is a resampling technique used for estimating the sampling distribution of a statistic by repeatedly sampling from the dataset with replacement. The steps involved in bootstrap are:\n",
    "   1. Randomly select a sample from the dataset with replacement.\n",
    "   2. Compute the statistic of interest (e.g., mean, median) on the sampled data.\n",
    "   3. Repeat steps 1 and 2 a large number of times (typically thousands of times).\n",
    "   4. Calculate the desired statistic (e.g., confidence interval) from the distribution of the resampled statistics.\n",
    "\n",
    "Q9. To estimate the 95% confidence interval for the population mean height using bootstrap:\n",
    "   - Sample with replacement from the observed sample of 50 tree heights.\n",
    "   - Compute the mean of each bootstrap sample.\n",
    "   - Repeat this process thousands of times to obtain a distribution of sample means.\n",
    "   - Calculate the 2.5th and 97.5th percentiles of this distribution to form the 95% confidence interval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
