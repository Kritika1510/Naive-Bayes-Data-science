{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. In Gradient Boosting, a weak learner typically refers to a simple model that performs slightly better than random chance on a given task. Common examples include decision trees with shallow depth (stumps or small trees), linear models, or even simple rules. These weak learners are often referred to as \"base learners\" or \"base models.\"\n",
    "\n",
    "Q5. The intuition behind the Gradient Boosting algorithm is to sequentially build a series of weak learners to correct the errors made by the previous ones. It works by fitting the new model to the residuals (the differences between the actual and predicted values) of the previous model. In essence, each new weak learner focuses on the mistakes made by the ensemble of models built so far, gradually reducing the overall error.\n",
    "\n",
    "Q6. The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. After initializing the ensemble with a simple model, such as the mean of the target variable, it iteratively adds new models to correct the errors made by the ensemble so far. Each new model is trained on the residuals (the differences between the actual and predicted values) of the ensemble's predictions from the previous iteration.\n",
    "\n",
    "Q7. The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are as follows\n",
    "    Initialize the model: Start with a simple model, typically the mean of the target variable, as the initial prediction.\n",
    "\n",
    "    Compute residuals: Calculate the residuals by subtracting the initial prediction from the actual target values.\n",
    "    \n",
    "    Fit a weak learner to the residuals**: Train a weak learner (e.g., a decision tree with shallow depth) to predict the residuals.\n",
    "  \n",
    "    Update the model: Add the predictions of the weak learner to the ensemble, adjusting the predictions based on a learning rate (shrinkage parameter).\n",
    "\n",
    "    Final prediction: The final prediction is obtained by summing the predictions of all weak learners, optionally weighted by their contribution to the ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
