{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as:\n",
    "\n",
    "\\[ f(x) = w^T x + b \\]\n",
    "\n",
    "Where:\n",
    "- \\( f(x) \\) represents the decision function.\n",
    "- \\( x \\) is the input vector.\n",
    "- \\( w \\) is the weight vector.\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "Q2. The objective function of a linear SVM aims to maximize the margin between the two classes while minimizing the classification error. Mathematically, it can be formulated as:\n",
    "\n",
    "\\[ \\min_{w,b} \\frac{1}{2} ||w||^2 \\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[ y_i(w^T x_i + b) \\geq 1 \\quad \\text{for all training samples } (x_i, y_i) \\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\) and \\( b \\) are the parameters of the hyperplane.\n",
    "- \\( ||w|| \\) is the Euclidean norm of the weight vector.\n",
    "- \\( (x_i, y_i) \\) are the training samples and their corresponding labels.\n",
    "- The constraint ensures that each training sample is correctly classified and lies on the correct side of the decision boundary with a margin of at least 1.\n",
    "\n",
    "Q3. The kernel trick in SVM is a method to implicitly map the input vectors into higher-dimensional feature spaces without actually computing the transformed feature vectors explicitly. This allows SVMs to efficiently handle non-linear decision boundaries by operating in the higher-dimensional space where the data may be linearly separable. The kernel function calculates the dot product of the transformed feature vectors in the higher-dimensional space without explicitly computing the transformation.\n",
    "\n",
    "Q4. Support vectors are the data points that lie closest to the decision boundary (hyperplane) and influence the position and orientation of the hyperplane. They are the critical elements of SVM as they determine the location of the decision boundary and are essential for making predictions. In essence, support vectors are the data points that would change the position of the decision boundary if removed. For example, consider a binary classification problem with two classes separated by a hyperplane. The support vectors are the data points from each class that lie closest to the hyperplane.\n",
    "\n",
    "Q5. Here's an illustration with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM:\n",
    "\n",
    "- **Hyperplane**: In a linear SVM, the hyperplane is the decision boundary that separates the classes. It is defined by the equation \\( w^T x + b = 0 \\), where \\( w \\) is the weight vector and \\( b \\) is the bias term. Example: Consider a binary classification problem with two classes (blue and red points). The hyperplane is the line that best separates the two classes.\n",
    "\n",
    "- **Marginal Plane**: The marginal plane is parallel to the hyperplane and passes through the support vectors. It defines the margin, which is the distance between the hyperplane and the closest data points from each class (support vectors). Example: In a two-dimensional space, the marginal plane consists of two parallel lines equidistant from the hyperplane and passing through the support vectors.\n",
    "\n",
    "- **Soft Margin**: In soft-margin SVM, the margin is allowed to be violated by some data points to achieve better generalization. This is useful when the data is not perfectly separable. Example: When the data points are not linearly separable, the soft-margin SVM allows for some misclassifications to find a decision boundary with a larger margin.\n",
    "\n",
    "- **Hard Margin**: In hard-margin SVM, no data points are allowed to violate the margin, which means the data must be linearly separable. Example: When the data is perfectly separable, the hard-margin SVM finds the decision boundary with the maximum margin without any misclassifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
