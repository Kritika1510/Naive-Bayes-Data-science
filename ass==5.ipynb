{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7547b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Random Forest Regressor is an ensemble learning method based on the Random Forest algorithm, designed for regression tasks. It consists of a collection of decision trees where each tree is trained on a random subset of the training data and a random subset of the features.\n",
    "\n",
    "Q2. Random Forest Regressor reduces the risk of overfitting by training each decision tree on a random subset of the training data and features. Additionally, the randomness introduced during the tree building process, such as random feature selection and bootstrapping, helps to decorrelate the individual trees, leading to a more robust ensemble model.\n",
    "\n",
    "Q3. Random Forest Regressor aggregates the predictions of multiple decision trees by averaging their predictions (for regression tasks). Each decision tree in the ensemble independently makes a prediction, and the final prediction is obtained by averaging these predictions across all trees.\n",
    "\n",
    "Q4. Some of the hyperparameters of Random Forest Regressor include:\n",
    "   - Number of trees (n_estimators)\n",
    "   - Maximum depth of each tree (max_depth)\n",
    "   - Minimum number of samples required to split an internal node (min_samples_split)\n",
    "   - Minimum number of samples required to be at a leaf node (min_samples_leaf)\n",
    "   - Maximum number of features to consider when looking for the best split (max_features)\n",
    "\n",
    "Q5. The main difference between Random Forest Regressor and Decision Tree Regressor lies in their underlying algorithms and the way they handle randomness and overfitting. While Decision Tree Regressor builds a single decision tree, potentially prone to overfitting, Random Forest Regressor builds multiple decision trees and aggregates their predictions to improve generalization performance and reduce overfitting.\n",
    "\n",
    "Q6. Advantages of Random Forest Regressor include:\n",
    "   - Reduced risk of overfitting compared to individual decision trees\n",
    "   - Good performance with default hyperparameters\n",
    "   - Ability to handle high-dimensional data and large datasets\n",
    "   - Robustness to outliers and noise in the data\n",
    "\n",
    "     disadvanages include:\n",
    "   - Less interpretable compared to individual decision trees\n",
    "   - Increased computational complexity compared to single decision trees\n",
    "   - May require tuning of hyperparameters for optimal performance\n",
    "\n",
    "Q7. The output of Random Forest Regressor is a continuous value, representing the predicted target variable for each input sample.\n",
    "\n",
    "Q8. Yes, Random Forest Regressor can be adapted for classification tasks as well by using it as Random Forest Classifier. In classification tasks, the output is a class label or probability distribution over classes, rather than a continuous value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
