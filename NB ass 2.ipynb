{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let:\n",
    "- \\( A \\) be the event that an employee is a smoker.\n",
    "- \\( B \\) be the event that an employee uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "- \\( P(B) \\), the probability that an employee uses the health insurance plan, which is 70% or 0.70.\n",
    "- \\( P(A|B) \\), the conditional probability that an employee is a smoker given that they use the health insurance plan, which is 40% or 0.40.\n",
    "\n",
    "We want to find \\( P(A|B) \\).\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Given that \\( P(B|A) = 0.40 \\) and \\( P(B) = 0.70 \\), we can calculate \\( P(A) \\) using the complement rule:\n",
    "\\[ P(A) = 1 - P(\\text{not } A) \\]\n",
    "\\[ P(A) = 1 - P(\\text{not } A|\\text{not } B) \\]\n",
    "\\[ P(A) = 1 - (1 - P(A|\\text{not } B)) \\]\n",
    "\\[ P(A) = P(A|\\text{not } B) \\]\n",
    "\n",
    "Since we're not given \\( P(A|\\text{not } B) \\) directly, we can't calculate \\( P(A) \\) from the information given.\n",
    "\n",
    "Q2. The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed for:\n",
    "- Bernoulli Naive Bayes is typically used when the features are binary (i.e., present or absent).\n",
    "- Multinomial Naive Bayes is used when the features are categorical and can take on multiple discrete values.\n",
    "\n",
    "Q3. Bernoulli Naive Bayes handles missing values by ignoring them during the calculation of probabilities. If a feature's value is missing for a particular instance, it is treated as if the feature is not present (i.e., the binary feature is set to 0). This assumption simplifies the calculation but may not always reflect the true nature of the data.\n",
    "\n",
    "Q4. Yes, Gaussian Naive Bayes can be used for multi-class classification. In Gaussian Naive Bayes, each class is assumed to be generated from a Gaussian distribution, characterized by its mean and variance. During classification, the likelihood of a given feature value belonging to each class is computed using the Gaussian probability density function. The class with the highest likelihood is then predicted as the output. This method can be extended to handle multiple classes by comparing the likelihoods for each class and selecting the class with the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
